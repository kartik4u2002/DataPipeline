# ğŸ§© Real-Time Data Pipeline â€” Kafka + Python + MongoDB Atlas

This project is a **real-time data pipeline** built using **Kafka (Python API)** and **MongoDB Atlas**.  
It continuously fetches live data from **NewsAPI**, processes and transforms it using Python, and then streams the refined data to **MongoDB Atlas** for storage and analysis.

---

## ğŸš€ Overview

This pipeline demonstrates how to:
1. Fetch **real-time data** from an external API (NewsAPI).
2. Perform **data transformation and cleaning** using Python.
3. Stream the transformed data to a **Kafka topic**.
4. Consume the data from Kafka and store it in **MongoDB Atlas**.

---

## âš™ï¸ Architecture

NewsAPI â†’ Python Producer â†’ Kafka Topic â†’ Python Consumer â†’ MongoDB Atlas


Each stage of the pipeline is modular and can be scaled or modified independently.

---

## ğŸ§  Features

- Real-time news ingestion from [NewsAPI](https://newsapi.org/).  
- Lightweight data transformation in Python.  
- Asynchronous message passing via Kafka topics.  
- Persistent storage in MongoDB Atlas.  
- Environment-based configuration for flexibility.  
- Clean, modular code structure.

---

## ğŸ§° Tech Stack

- **Language:** Python  
- **Message Broker:** Apache Kafka  
- **Database:** MongoDB Atlas  
- **API Source:** NewsAPI  
- **Libraries:** `kafka-python`, `requests`, `pymongo`, `python-dotenv`

---

## ğŸ§© Project Structure

â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ producer.py # Fetches and publishes data to Kafka
â”‚ â”œâ”€â”€ consumer.py # Consumes data and stores in MongoDB
â”‚ â”œâ”€â”€ transformer.py # Handles data transformation
â”‚ â”œâ”€â”€ config.py # Loads environment variables
â”‚ â””â”€â”€ utils.py # Helper functions (optional)
â””â”€â”€ docker-compose.yml # For running Kafka locally (optional)


